{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n# 53907afe-531b-11ea-a595-00505601122b\n# b7ea974c-d389-11e8-a4be-00505601122b","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-18T16:12:02.847577Z","iopub.execute_input":"2022-05-18T16:12:02.848037Z","iopub.status.idle":"2022-05-18T16:12:02.866806Z","shell.execute_reply.started":"2022-05-18T16:12:02.847946Z","shell.execute_reply":"2022-05-18T16:12:02.866023Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 1. Setup","metadata":{}},{"cell_type":"markdown","source":"## 1.1. FS/OS Requirements","metadata":{}},{"cell_type":"code","source":"!cp /kaggle/input/sentiment-analysis/text_classification_dataset.py /kaggle/working/text_classification_dataset.py\n!cp -r /kaggle/input/sentiment-analysis/czech_facebook /kaggle/working/\n!tree /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:11:12.668608Z","iopub.execute_input":"2022-05-19T12:11:12.668891Z","iopub.status.idle":"2022-05-19T12:11:14.626726Z","shell.execute_reply.started":"2022-05-19T12:11:12.668860Z","shell.execute_reply":"2022-05-19T12:11:14.625540Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#!pip install -U tensorflow-gpu==2.8 tensorflow-addons==0.16.1 tensorflow-probability==0.16.0 tensorflow-hub==0.12.0 scipy\n!pip freeze | grep tensorflow","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:03:38.286143Z","iopub.execute_input":"2022-05-18T12:03:38.286421Z","iopub.status.idle":"2022-05-18T12:03:41.844196Z","shell.execute_reply.started":"2022-05-18T12:03:38.286392Z","shell.execute_reply":"2022-05-18T12:03:41.843387Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!grep -c ^processor /proc/cpuinfo\n!grep ^cpu\\\\scores /proc/cpuinfo | uniq |  awk '{print $4}'","metadata":{"execution":{"iopub.status.busy":"2022-05-18T12:03:44.866739Z","iopub.execute_input":"2022-05-18T12:03:44.867504Z","iopub.status.idle":"2022-05-18T12:03:46.218713Z","shell.execute_reply.started":"2022-05-18T12:03:44.867467Z","shell.execute_reply":"2022-05-18T12:03:46.217916Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Python imports","metadata":{}},{"cell_type":"code","source":"import argparse\nimport datetime\nimport functools\nimport os\nimport re\n\nos.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")  # Report only TF errors by default\n\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport wandb\nfrom wandb.keras import WandbCallback\n\ntry:\n    import transformers\nexcept Exception:\n    raise RuntimeError(\"You need to install the `transformers` package\")\n\nfrom text_classification_dataset import TextClassificationDataset","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:11:21.059763Z","iopub.execute_input":"2022-05-19T12:11:21.060039Z","iopub.status.idle":"2022-05-19T12:11:29.603365Z","shell.execute_reply.started":"2022-05-19T12:11:21.060008Z","shell.execute_reply":"2022-05-19T12:11:29.602596Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## 1.3. Args","metadata":{}},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\nparser.add_argument(\"--batch_size\", default=None, type=int, help=\"Batch size.\")\nparser.add_argument(\"--epochs\", default=None, type=int, help=\"Number of epochs.\")\nparser.add_argument(\"--seed\", default=42, type=int, help=\"Random seed.\")\nparser.add_argument(\"--threads\", default=1, type=int, help=\"Maximum number of threads to use.\")\nparser.add_argument(\"--buffer_size\", default=None, type=int, help=\"Dataset buffer size to load into memory. By default load the whole dataset.\")\nparser.add_argument(\"--checkpoints_period\", default=None, type=int, help=\"Checkpoint callback period.\")\nparser.add_argument(\"--stopping_patience\", default=None, type=int, help=\"Early stopping epochs patience.\")\nparser.add_argument(\"--label_smoothing\", default=None, type=float, help=\"\")\nparser.add_argument(\"--learning_rate\", default=0.01, type=float, help=\"Initial model learning rate.\")\nparser.add_argument(\"--decay_steps\", default=None, type=int, help=\"Decay steps for cosine decay\")\n\nargs = parser.parse_args([\n    '--threads=2',\n    '--batch_size=32',\n    '--epochs=10',\n    '--checkpoints_period=3',\n    '--stopping_patience=3',\n    '--learning_rate=0.00005',\n    '--label_smoothing=0.1',\n] if \"__file__\" not in globals() else None)\n\n# Create logdir name\nargs.logdir = os.path.join(\n    \"logs\",\n    \"{}-{}-{}\".format(\n        os.path.basename(globals().get(\"__file__\", \"notebook\")),\n        datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M%S\"),\n        \",\".join(\n            (\n                \"{}={}\".format(re.sub(\"(.)[^_]*_?\", r\"\\1\", k), v)\n                for k, v in sorted(vars(args).items())\n            )\n        ),\n    ),\n)\n\ntf.random.set_seed(args.seed) # tf2.6 (I have gpu issues on tf2.8 unfortunately)\ntf.config.threading.set_inter_op_parallelism_threads(args.threads)\ntf.config.threading.set_intra_op_parallelism_threads(args.threads)\n\nif args.decay_steps:\n    args.learning_rate = tf.keras.optimizers.schedules.CosineDecay(args.learning_rate, args.decay_steps)\nelif 'facebook' in vars():\n    pass\n    #args.decay_steps = tf.\n\n\nif args.buffer_size is None and 'facebook' in vars():\n    args.buffer_size = len(facebook.train)\n\nargs","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:11:33.538055Z","iopub.execute_input":"2022-05-19T12:11:33.538567Z","iopub.status.idle":"2022-05-19T12:11:33.563894Z","shell.execute_reply.started":"2022-05-19T12:11:33.538529Z","shell.execute_reply":"2022-05-19T12:11:33.563198Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 1.4. WanDB","metadata":{}},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:11:43.155505Z","iopub.execute_input":"2022-05-19T12:11:43.155796Z","iopub.status.idle":"2022-05-19T12:11:55.073674Z","shell.execute_reply.started":"2022-05-19T12:11:43.155765Z","shell.execute_reply":"2022-05-19T12:11:55.072970Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"run = wandb.init(project='sentiment_analysis_school_assignment',\n                 resume='allow',\n                 config={\n                     **vars(args),\n                     #\"loss_function\": \"sparse_categorical_crossentropy\",\n                     #\"architecture\": \"seq2seq-with-cnn-encoder-and-attention\",\n                     \"dataset\": \"czech_facebook\"\n                 })","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:12:23.241284Z","iopub.execute_input":"2022-05-19T12:12:23.242057Z","iopub.status.idle":"2022-05-19T12:12:32.500914Z","shell.execute_reply.started":"2022-05-19T12:12:23.242020Z","shell.execute_reply":"2022-05-19T12:12:32.500208Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 2. Data","metadata":{}},{"cell_type":"code","source":"tokenizer = transformers.AutoTokenizer.from_pretrained(\"ufal/eleczech-lc-small\", padding=True)\ntokenizer","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:12:37.483685Z","iopub.execute_input":"2022-05-19T12:12:37.484021Z","iopub.status.idle":"2022-05-19T12:12:42.673873Z","shell.execute_reply.started":"2022-05-19T12:12:37.483987Z","shell.execute_reply":"2022-05-19T12:12:42.673140Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"facebook = TextClassificationDataset(\n    \"czech_facebook\",\n    tokenizer=tokenizer if 'tokenizer' in vars() or 'tokenizer' in globals() else None\n)\n\nif args.buffer_size is None:\n    args.buffer_size = len(facebook.train.dataset)\n\nprint(\"Num labels: \", facebook.train.label_mapping.vocabulary_size())\nprint(\"Train size: \", len(facebook.train.dataset))\nprint(\"Dev size: \", len(facebook.dev.dataset))\nprint(\"Test size: \", len(facebook.test.dataset))","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:12:48.847748Z","iopub.execute_input":"2022-05-19T12:12:48.848207Z","iopub.status.idle":"2022-05-19T12:12:56.937945Z","shell.execute_reply.started":"2022-05-19T12:12:48.848172Z","shell.execute_reply":"2022-05-19T12:12:56.937146Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"facebook.train.data.keys()","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:05:04.311597Z","iopub.execute_input":"2022-05-18T10:05:04.312304Z","iopub.status.idle":"2022-05-18T10:05:04.318318Z","shell.execute_reply.started":"2022-05-18T10:05:04.312266Z","shell.execute_reply":"2022-05-18T10:05:04.317674Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for x, y in zip(facebook.train.data[\"documents\"][:10], facebook.train.data[\"labels\"][:10]):\n    print(\"{}: {}\".format(y, x))","metadata":{"execution":{"iopub.status.busy":"2022-05-18T11:58:24.747283Z","iopub.execute_input":"2022-05-18T11:58:24.747974Z","iopub.status.idle":"2022-05-18T11:58:24.765564Z","shell.execute_reply.started":"2022-05-18T11:58:24.747935Z","shell.execute_reply":"2022-05-18T11:58:24.763240Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for x in facebook.train.data[\"tokens\"][:10]:\n    print(x)","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:05:07.093704Z","iopub.execute_input":"2022-05-18T10:05:07.094611Z","iopub.status.idle":"2022-05-18T10:05:07.103332Z","shell.execute_reply.started":"2022-05-18T10:05:07.094560Z","shell.execute_reply":"2022-05-18T10:05:07.102427Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"print(\n    facebook.test.label_mapping([\"p\", \"n\", \"0\"]),\n    facebook.train.label_mapping([\"p\", \"n\", \"0\"]),\n    facebook.dev.label_mapping([\"p\", \"n\", \"0\"])\n)","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.histplot(facebook.train.data[\"labels\"], kde=True, discrete=True).set_title(\"Train labels distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:05:11.960819Z","iopub.execute_input":"2022-05-18T10:05:11.961759Z","iopub.status.idle":"2022-05-18T10:05:12.288164Z","shell.execute_reply.started":"2022-05-18T10:05:11.961720Z","shell.execute_reply":"2022-05-18T10:05:12.287021Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.histplot([len(x) for x in facebook.train.data[\"documents\"]], kde=True, discrete=True).set_title(\"Train documents length distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:05:14.834826Z","iopub.execute_input":"2022-05-18T10:05:14.835169Z","iopub.status.idle":"2022-05-18T10:05:15.336174Z","shell.execute_reply.started":"2022-05-18T10:05:14.835136Z","shell.execute_reply":"2022-05-18T10:05:15.335266Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.histplot([len(x) for x in facebook.dev.data[\"documents\"]], kde=True, discrete=True).set_title(\"Dev documents length distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:05:18.004324Z","iopub.execute_input":"2022-05-18T10:05:18.004997Z","iopub.status.idle":"2022-05-18T10:05:18.441650Z","shell.execute_reply.started":"2022-05-18T10:05:18.004956Z","shell.execute_reply":"2022-05-18T10:05:18.440614Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,7))\nsns.histplot([len(x) for x in facebook.test.data[\"documents\"]], kde=True, discrete=True).set_title(\"Test documents length distribution\")","metadata":{"execution":{"iopub.status.busy":"2022-05-18T10:05:21.474611Z","iopub.execute_input":"2022-05-18T10:05:21.475076Z","iopub.status.idle":"2022-05-18T10:05:22.184343Z","shell.execute_reply.started":"2022-05-18T10:05:21.475043Z","shell.execute_reply":"2022-05-18T10:05:22.183330Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"tf.one_hot([2,3,4], 5)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:05:18.585853Z","iopub.execute_input":"2022-05-19T12:05:18.586443Z","iopub.status.idle":"2022-05-19T12:05:18.599431Z","shell.execute_reply.started":"2022-05-19T12:05:18.586404Z","shell.execute_reply":"2022-05-19T12:05:18.598563Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def create_dataset(name):\n    \n    dataset_spec = getattr(facebook, name)\n    data = tokenizer(dataset_spec.data[\"documents\"], padding=True, return_tensors=\"tf\")\n    labels = tf.cast(\n        dataset_spec.label_mapping(\n            dataset_spec.data[\"labels\"] if name != \"test\" else tf.fill([len(dataset_spec.dataset)], \"0\")\n        ),\n        tf.int32\n    )\n    \n    if args.label_smoothing:\n        labels = tf.one_hot(labels, dataset_spec.label_mapping.vocabulary_size())\n    \n    dataset = tf.data.Dataset.from_tensor_slices((dict(data), labels))\n    dataset = dataset.shuffle(args.buffer_size, seed=args.seed) if name == \"train\" else dataset\n    dataset = dataset.batch(args.batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    return dataset\n\ntrain, dev, test = create_dataset(\"train\"), create_dataset(\"dev\"), create_dataset(\"test\")\nprint(train, '\\n', dev, '\\n', test)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:13:21.857667Z","iopub.execute_input":"2022-05-19T12:13:21.857943Z","iopub.status.idle":"2022-05-19T12:13:22.812122Z","shell.execute_reply.started":"2022-05-19T12:13:21.857913Z","shell.execute_reply":"2022-05-19T12:13:22.811420Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# 3. Model","metadata":{}},{"cell_type":"code","source":"eleczech = transformers.TFAutoModelForSequenceClassification.from_pretrained(\"ufal/eleczech-lc-small\", num_labels=facebook.train.label_mapping.vocabulary_size())\neleczech","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:13:30.188894Z","iopub.execute_input":"2022-05-19T12:13:30.189653Z","iopub.status.idle":"2022-05-19T12:13:36.286860Z","shell.execute_reply.started":"2022-05-19T12:13:30.189614Z","shell.execute_reply":"2022-05-19T12:13:36.286152Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"metrics = ['accuracy']\n\nif args.label_smoothing:\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=args.label_smoothing)\n    metrics.append('categorical_accuracy')\nelse:\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    metrics.append('sparse_categorical_accuracy')\n\neleczech.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=args.learning_rate),\n    loss=loss,\n    metrics=metrics\n)\neleczech.summary()","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:13:50.621511Z","iopub.execute_input":"2022-05-19T12:13:50.621784Z","iopub.status.idle":"2022-05-19T12:13:50.656486Z","shell.execute_reply.started":"2022-05-19T12:13:50.621756Z","shell.execute_reply":"2022-05-19T12:13:50.655741Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"eleczech.fit(\n    train,\n    validation_data=dev,\n    epochs=args.epochs,\n    shuffle=False,\n    callbacks=[\n        WandbCallback(labels=facebook.train.label_mapping.get_vocabulary(), save_model=False, validation_data=dev)\n    ]\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:14:07.288110Z","iopub.execute_input":"2022-05-19T12:14:07.288393Z","iopub.status.idle":"2022-05-19T12:17:48.005068Z","shell.execute_reply.started":"2022-05-19T12:14:07.288363Z","shell.execute_reply":"2022-05-19T12:17:48.003875Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"label_strings = facebook.test.label_mapping.get_vocabulary()\npredictiosn = tf.nn.softmax(eleczech.predict(test.take(1)).logits, axis=-1)\nfor sentence in predictiosn:\n    print(label_strings[np.argmax(sentence)])","metadata":{"execution":{"iopub.status.busy":"2022-05-19T10:34:31.725859Z","iopub.execute_input":"2022-05-19T10:34:31.726382Z","iopub.status.idle":"2022-05-19T10:34:31.811113Z","shell.execute_reply.started":"2022-05-19T10:34:31.726345Z","shell.execute_reply":"2022-05-19T10:34:31.810331Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"#os.makedirs(args.logdir, exist_ok=True)\nwith open(\"/kaggle/working/sentiment_analysis.txt\", \"w\", encoding=\"utf-8\") as predictions_file:\n    # TODO: Predict the tags on the test set.\n    predictions = eleczech.predict(test).logits # We dont need to aply softmax since argmax(logits) = argmax(softmax(logits))\n\n    label_strings = facebook.test.label_mapping.get_vocabulary()\n    for sentence in predictions:\n        print(label_strings[np.argmax(sentence)], file=predictions_file)","metadata":{"execution":{"iopub.status.busy":"2022-05-19T12:20:58.792068Z","iopub.execute_input":"2022-05-19T12:20:58.793044Z","iopub.status.idle":"2022-05-19T12:21:02.346838Z","shell.execute_reply.started":"2022-05-19T12:20:58.792999Z","shell.execute_reply":"2022-05-19T12:21:02.346018Z"},"trusted":true},"execution_count":14,"outputs":[]}]}