### Lecture: 9. Seq2seq, NMT, Transformer
#### Date: Apr 11
#### Slides: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2122/slides/?09
#### Reading: https://ufal.mff.cuni.cz/~straka/courses/npfl114/2122/slides.pdf/npfl114-09.pdf,PDF Slides
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-09-czech.mp4, CZ Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-09-english.mp4, EN Lecture
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-09-czech.practicals.mp4, CZ Practicals
#### Video: https://lectures.ms.mff.cuni.cz/video/rec/npfl114/2122/npfl114-09-english.practicals.mp4, EN Practicals
#### Questions: #lecture_9_questions
#### Lecture assignment: lemmatizer_noattn
#### Lecture assignment: lemmatizer_attn
#### Lecture assignment: lemmatizer_competition

- Neural Machine Translation using Encoder-Decoder or Sequence-to-Sequence architecture [Section 12.5.4 of DLB, [Ilya Sutskever, Oriol Vinyals, Quoc V. Le: **Sequence to Sequence Learning with Neural Networks**](https://arxiv.org/abs/1409.3215) and [Kyunghyun Cho et al.: **Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation**](https://arxiv.org/abs/1406.1078)]
- Using Attention mechanism in Neural Machine Translation [Section 12.4.5.1 of DLB, [Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio: **Neural Machine Translation by Jointly Learning to Align and Translate**](https://arxiv.org/abs/1409.0473)]
- Translating Subword Units [[Rico Sennrich, Barry Haddow, Alexandra Birch: **Neural Machine Translation of Rare Words with Subword Units**](https://arxiv.org/abs/1508.07909)]
- _Google NMT [[Yonghui Wu et al.: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)]_
- Transformer architecture [[Attention Is All You Need](https://arxiv.org/abs/1706.03762)]
