title: NPFL114, Lecture 13
class: title, langtech, cc-by-nc-sa

# Introduction to Deep Reinforcement Learning

## Milan Straka

### May 9, 2022

---
section: RL
class: center, middle
# Reinforcement Learning

# Reinforcement Learning

---
# History of Reinforcement Learning

_Develop goal-seeking agent trained using reward signal._

~~~
- _Optimal control_ in 1950s – Richard Bellman

~~~
- Trial and error learning – since 1850s
  - Law and effect – Edward Thorndike, 1911
    - Responses that produce a satisfying effect in a particular situation become
      more likely to occur again in that situation, and responses that produce
      a discomforting effect become less likely to occur again in that situation
  - Shannon, Minsky, Clark&Farley, … – 1950s and 1960s
  - Tsetlin, Holland, Klopf – 1970s
  - Sutton, Barto – since 1980s

~~~
- Arthur Samuel – first implementation of temporal difference methods
  for playing checkers

~~~
## Notable successes
- Gerry Tesauro – 1992, human-level Backgammon program trained solely by self-play

~~~
- IBM Watson in Jeopardy – 2011

---
# History of Reinforcement Learning – Recent successes

- Human-level [video game playing](https://www.youtube.com/watch?v=pzYZvYhGxH8&list=PL2D_SqpHWZGgQu4gIUARUWk3rrwBrBEgq) (DQN) – 2013 (2015 Nature), Mnih. et al, Deepmind

  - 29 games out of 49 comparable or better to professional game players
  - 8 days on GPU
  - human-normalized mean: 121.9%, median: 47.5% on 57 games

~~~
- A3C – 2016, Mnih. et al
  - 4 days on 16-threaded CPU
  - human-normalized mean: 623.0%, median: 112.6% on 57 games

~~~
- Rainbow – 2017
  - human-normalized median: 153%; ~39 days of game play experience

~~~
- Impala – Feb 2018
  - one network and set of parameters to rule them all
  - human-normalized mean: 176.9%, median: 59.7% on 57 games

~~~
- PopArt-Impala – Sep 2018
  - human-normalized median: 110.7% on 57 games; 57*38.6 days of experience

---
# History of Reinforcement Learning – Recent successes

![w=29%,f=right](r2d2_results.svgz)

- R2D2 – Jan 2019

  - human-normalized mean: 4024.9%, median: 1920.6% on 57 games
  - processes ~5.7B frames during a day of training
~~~
- MuZero – Nov 2019
  - planning with a learned model: 4999.2%, median: 2041.1%
~~~
- Agent57 - Mar 2020
  - super-human performance on all 57 Atari games
~~~
![w=69%,mw=30%,h=center,f=right](der-progress.svgz)

- Data-efficient Rainbow – Jun 2019
  - learning from ~2 hours of game experience

---
# History of Reinforcement Learning – Recent successes

- AlphaGo

  - Mar 2016 – beat 9-dan professional player Lee Sedol
~~~
- AlphaGo Master – Dec 2016
  - beat 60 professionals, beat Ke Jie in May 2017
~~~
- AlphaGo Zero – 2017
  - trained only using self-play
  - surpassed all previous version after 40 days of training
~~~
- AlphaZero – Dec 2017 (Dec 2018 in Nature)
  - self-play only, defeated AlphaGo Zero after 30 hours of training
  - impressive chess and shogi performance after 9h and 12h, respectively
![w=30%,h=center](a0_results.svgz)

---
# History of Reinforcement Learning – Recent successes

- Dota2 – Aug 2017

  - won 1v1 matches against a professional player

~~~
- MERLIN – Mar 2018
  - unsupervised representation of states using external memory
  - beat human in unknown maze navigation

~~~
- FTW – Jul 2018
  - beat professional players in two-player-team Capture the flag FPS
  - solely by self-play, trained on 450k games

~~~
- OpenAI Five – Aug 2018
  - won 5v5 best-of-three match against professional team
  - 256 GPUs, 128k CPUs, 180 years of experience per day

~~~
- AlphaStar
  - Jan 2019: won 10 out of 11 StarCraft II games against two professional players
  - Oct 2019: ranked 99.8% on `Battle.net`, playing with full game rules

~~~
- Multiagent hide and seek: https://openai.com/blog/emergent-tool-use/

---
# History of Reinforcement Learning – Recent successes

- Neural Architecture Search – since 2017

  - automatically designing CNN image recognition networks
    surpassing state-of-the-art performance
  - NasNet, EfficientNet, EfficientNetV2, …
~~~
  - AutoML: automatically discovering
    - architectures (CNN, RNN, overall topology)
    - activation functions
    - optimizers
    - …

~~~
- Optimize non-differentiable loss
~~~
  - improved translation quality in 2016

~~~
- Discovering discrete latent structures
~~~
- Controlling cooling in Google datacenters directly by AI (2018)
  - reaching 30% cost reduction

---
section: Multi-armed Bandits
# Multi-armed Bandits

![w=50%,h=center,v=middle](one-armed-bandit.jpg)

---
class: middle
# Multi-armed Bandits

![w=70%,h=center,v=middle](k-armed_bandits.svgz)

---
# Multi-armed Bandits

We start by selecting action $A_1$, which is the index of the arm to use, and we
get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, …

~~~
Let $q_*(a)$ be the real **value** of an action $a$:
$$q_*(a) = 𝔼[R_t | A_t = a].$$

~~~

Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking
trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to
estimate $Q_t(a)$ is
$$Q_t(a) ≝ \frac{\textrm{sum of rewards when action }a\textrm{ is taken}}{\textrm{number of times action }a\textrm{ was taken}}.$$

~~~
Following the definition of $Q_t(a)$, we could choose a **greedy** action $A_t$ as
$$A_t ≝ \argmax_a Q_t(a).$$

---
section: $ε$-greedy
# $ε$-greedy Method

## Exploitation versus Exploration

Choosing a greedy action is **exploitation** of current estimates. We however also
need to **explore** the space of actions to improve our estimates.

~~~

An _$ε$-greedy_ method follows the greedy action with probability $1-ε$, and
chooses a uniformly random action with probability $ε$.

---
# $ε$-greedy Method

![w=52%,h=center,v=middle](e_greedy.svgz)

---
# $ε$-greedy Method

## Incremental Implementation

Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \ldots, R_n$.

$$\begin{aligned}
Q_{n+1} &= \frac{1}{n} ∑_{i=1}^n R_i \\
    &= \frac{1}{n} (R_n + \frac{n-1}{n-1} ∑_{i=1}^{n-1} R_i) \\
    &= \frac{1}{n} (R_n + (n-1) Q_n) \\
    &= \frac{1}{n} (R_n + n Q_n - Q_n) \\
    &= Q_n + \frac{1}{n}\Big(R_n - Q_n\Big)
\end{aligned}$$

---
# $ε$-greedy Method Algorithm

![w=100%,v=middle](bandits_algorithm.svgz)

---
section: MDP
# Markov Decision Process

![w=85%,h=center,v=middle](diagram.svgz)

~~~~
# Markov Decision Process

![w=53%,h=center](diagram.svgz)

A **Markov decision process** (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,
where:
- $𝓢$ is a set of states,
~~~
- $𝓐$ is a set of actions,
~~~
- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that
  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a **reward** $r ∈ ℝ$,
~~~
- $γ ∈ [0, 1]$ is a **discount factor** (we will always use $γ=1$ and finite episodes in this course).

~~~
Let a **return** $G_t$ be $G_t ≝ ∑_{k=0}^∞ γ^k R_{t + 1 + k}$. The goal is to optimize $𝔼[G_0]$.

---
# Multi-armed Bandits as MDP

To formulate $n$-armed bandits problem as MDP, we do not need states.
Therefore, we could formulate it as:
- one-element set of states, $𝓢=\{S\}$;
~~~
- an action for every arm, $𝓐=\{a_1, a_2, …, a_n\}$;
~~~
- assuming every arm produces rewards with a distribution of $𝓝(μ_i, σ_i^2)$,
  the MDP dynamics function $p$ is defined as
  $$p(S, r | S, a_i) = 𝓝(r | μ_i, σ_i^2).$$

~~~
One possibility to introduce states in multi-armed bandits problem is to
consider a separate reward distribution for every state. Such generalization is
called **Contextualized Bandits** problem. Assuming state transitions are
independent on rewards and given by a distribution $\textit{next}(s)$, the MDP
dynamics function for contextualized bandits problem is given by
$$p(s', r | s, a_i) = 𝓝(r | μ_{i,s}, σ_{i,s}^2) ⋅ \textit{next}(s'|s).$$

---
# Episodic and Continuing Tasks

If the agent-environment interaction naturally breaks into independent
subsequences, usually called **episodes**, we talk about **episodic tasks**.
Each episode then ends in a special **terminal state**, followed by a reset
to a starting state (either always the same, or sampled from a distribution
of starting states).

~~~
In episodic tasks, it is often the case that every episode ends in at most
$H$ steps. These **finite-horizon tasks** then can use discount factor $γ=1$,
because the return $G ≝ ∑_{t=0}^H γ^t R_{t + 1}$ is well defined.

~~~
If the agent-environment interaction goes on and on without a limit, we instead
talk about **continuing tasks**. In this case, the discount factor $γ$ needs
to be sharply smaller than 1.

---
# (State-)Value and Action-Value Functions

A **policy** $π$ computes a distribution of actions in a given state, i.e.,
$π(a | s)$ corresponds to a probability of performing an action $a$ in state
$s$.

~~~
To evaluate a quality of a policy, we define **value function** $v_π(s)$, or
**state-value function**, as
$$\begin{aligned}
  v_π(s) & ≝ 𝔼_π\left[G_t \middle| S_t = s\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s\right] \\
         & = 𝔼_{A_t ∼ π(s)} 𝔼_{S_{t+1},R_{t+1} ∼ p(s,A_t)} \big[R_{t+1}
           + γ 𝔼_{A_{t+1} ∼ π(S_{t+1})} 𝔼_{S_{t+2},R_{t+2} ∼ p(S_{t+1},A_{t+1})} \big[R_{t+2} + … \big]\big]
\end{aligned}$$

~~~
An **action-value function** for a policy $π$ is defined analogously as
$$q_π(s, a) ≝ 𝔼_π\left[G_t \middle| S_t = s, A_t = a\right] = 𝔼_π\left[∑\nolimits_{k=0}^∞ γ^k R_{t+k+1} \middle| S_t=s, A_t = a\right].$$

~~~
The value function and state-value function can be of course expressed using one another:
$$v_π(s) = 𝔼_{a∼π}\big[q_π(s, a)\big],~~~~~~~q_π(s, a) = 𝔼_{s', r ∼ p}\big[r + γv_π(s')\big].$$

---
# Optimal Value Functions

Optimal state-value function is defined as
$$v_*(s) ≝ \max_π v_π(s),$$
~~~
analogously
$$q_*(s, a) ≝ \max_π q_π(s, a).$$

~~~
Any policy $π_*$ with $v_{π_*} = v_*$ is called an **optimal policy**. Such policy
can be defined as $π_*(s) ≝ \argmax_a q_*(s, a) = \argmax_a 𝔼[R_{t+1} + γv_*(S_{t+1}) | S_t = s, A_t = a]$.
When multiple actions maximize $q_*(s, a)$, the optimal policy can
stochastically choose any of them.

~~~
## Existence
In finite-horizon tasks or if $γ < 1$, there always exists a unique optimal
state-value function, a unique optimal action-value function, and a (not necessarily
unique) optimal policy.

---
section: MonteCarlo
# Monte Carlo Methods

We now present the first algorithm for computing optimal policies without assuming
a knowledge of the environment dynamics.

However, we still assume there are finitely many states $𝓢$, finitely many
actions $𝓐$ and we will store estimates for every possible state-action pair.

~~~
Monte Carlo methods are based on estimating returns from complete episodes.
Furthermore, if the model (of the environment) is not known, we need to
estimate returns for the action-value function $q$ instead of $v$.

---
# Monte Carlo and $ε$-soft Policies

For the estimates to converge to the real values, every reachable state must be
visited infinitely many times and every action in such a state must be selected
infinitely many times in limit.

~~~
A policy is called $ε$-soft, if
$$π(a|s) ≥ \frac{ε}{|𝓐(s)|}.$$
and we call it $ε$-greedy, if one action has a maximum probability of
$1-ε+\frac{ε}{|A(s)|}$.

~~~
It can be shown that when considering the class of $ε$-soft policies, one of the
optimal policies is always $ε$-greedy – we will therefore search among the
$ε$-greedy policies only.

---
# Monte Carlo for $ε$-soft Policies

### On-policy every-visit Monte Carlo for $ε$-soft Policies
Algorithm parameter: small $ε>0$

Initialize $Q(s, a) ∈ ℝ$ arbitrarily (usually to 0), for all $s ∈ 𝓢, a ∈ 𝓐$<br>
Initialize $C(s, a) ∈ ℤ$ to 0, for all $s ∈ 𝓢, a ∈ 𝓐$

Repeat forever (for each episode):
- Generate an episode $S_0, A_0, R_1, …, S_{T-1}, A_{T-1}, R_T$,
  by generating actions as follows:
  - With probability $ε$, generate a random uniform action
  - Otherwise, set $A_t ≝ \argmax\nolimits_a Q(S_t, a)$
- $G ← 0$
- For each $t=T-1, T-2, …, 0$:
  - $G ← γG + R_{t+1}$
  - $C(S_t, A_t) ← C(S_t, A_t) + 1$
  - $Q(S_t, A_t) ← Q(S_t, A_t) + \frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$

---
section: REINFORCE
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its **gradient** $∇_{→θ} v_π(s)$.

---
# Policy Gradient Theorem

Assume that $𝓢$ and $𝓐$ are finite, $γ=1$, and that maximum episode length $H$ is also finite.

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{s∼h} v_π(s)$.

~~~
Then
$$∇_{→θ} v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ)$$
and
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ),$$

~~~
where $P(s → … → s'|π)$ is the probability of getting to state $s'$ when starting
from state $s$, after any number of 0, 1, … steps.


---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ \big(∑\nolimits_{s'} p(s'|s, a)(r + v_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
                \quad\qquad\qquad ∑\nolimits_{a'} \Big[ ∇ π(a'|s'; →θ) q_π(s', a') + π(a'|s'; →θ) \big(∑\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\big) \Big] \Big) \Big) \Big]$

~~~
_Continuing to expand all $v_π(s'')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} ∑_{k=0}^H P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ).$

---
# Proof of Policy Gradient Theorem

To finish the proof of the first part, it is enough to realize that
$$∑\nolimits_{k=0}^H P(s → s'\textrm{~in~}k\textrm{~steps~}|π) ∝ P(s → … → s'|π).$$

~~~
For the second part, we know that
$$∇_{→θ} J(→θ) = 𝔼_{s ∼ h} ∇_{→θ} v_π(s) ∝ 𝔼_{s ∼ h} ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_{→θ} π(a | s'; →θ),$$
~~~
therefore using the fact that $μ(s') = 𝔼_{s ∼ h} P(s → … → s'|π)$ we get
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ).$$

~~~
Finally, note that the theorem can be proven with infinite $𝓢$ and $𝓐$; and
also for infinite episodes when discount factor $γ<1$.

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, minimizing $-J(→θ) ≝ -𝔼_{s∼h} v_π(s)$. The loss gradient is then
$$∇_{→θ} -J(→θ) ∝ -∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ) = -𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_{→θ} π(a | s; →θ).$$

~~~
However, the sum over all actions is problematic. Instead, we rewrite it to an
expectation which we can estimate by sampling:
$$∇_{→θ} -J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_{→θ} -\ln π(a | s; →θ),$$
where we used the fact that
$$∇_{→θ} \ln π(a | s; →θ) = \frac{1}{π(a | s; →θ)} ∇_{→θ} π(a | s; →θ).$$

---
# REINFORCE Algorithm

REINFORCE therefore minimizes the loss
$$𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_{→θ} -\ln π(a | s; →θ),$$
estimating the $q_π(s, a)$ by a single sample.

Note that the loss is just a weighted variant of negative log-likelihood (NLL),
where the sampled actions play a role of gold labels and are weighted according
to their return.

![w=75%,h=center](reinforce.svgz)

---
section: Baseline
# REINFORCE with Baseline

The returns can be arbitrary – better-than-average and worse-than-average
returns cannot be recognized from the absolute value of the return.

~~~
Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$
to
$$∇_{→θ} J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \big(q_π(s, a) - b(s)\big) ∇_{→θ} π(a | s; →θ).$$

~~~
The baseline $b(s)$ can be a function or even a random variable, as long as it
does not depend on $a$, because
$$∑_a b(s) ∇_{→θ} π(a | s; →θ) = b(s) ∑_a ∇_{→θ} π(a | s; →θ) = b(s) ∇_{→θ} ∑_a π(a | s; →θ) = b(s) ∇1 = 0.$$

---
# REINFORCE with Baseline

A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize the
variance of the gradient estimator. Such baseline reminds centering of the
returns, given that
$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$

~~~
Then, better-than-average returns are positive and worse-than-average returns
are negative.

~~~
The resulting $q_π(s, a) - v_π(s)$ function is also called the **advantage** function
$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$

~~~
Of course, the $v_π(s)$ baseline can be only approximated. If neural networks
are used to estimate $π(a|s; →θ)$, then some part of the network is usually
shared between the policy and value function estimation, which is trained using
mean square error of the predicted and observed return.

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline.svgz)

---
# REINFORCE with Baseline

![w=100%](reinforce_with_baseline_comparison.svgz)

---
section: AdvancedRL
# What Next

If you liked the introduction to the deep reinforcement learning, I have
a whole course **NPFL122 – Deep Reinforcement Learning** in the winter
semester.

~~~
- It covers a range of reinforcement learning algorithms, from the basic
  ones to more advanced algorithms utilizing deep neural networks.

~~~
- 2/2 C+Ex

~~~
- An elective (povinně volitelný) course in the programs:
  - Artificial Intelligence,
  - Language Technologies and Computational Linguistics.
