title: NPFL114, Lecture 14
class: title, langtech, cc-by-nc-sa

# NASNet, Speech Synthesis,<br>External Memory Networks

## Milan Straka

### May 16, 2022

---
section: NASNet
# Neural Architecture Search (NASNet) – 2017

- We can design neural network architectures using reinforcement learning.

~~~
- The designed network is encoded as a sequence of elements, and is generated
  using an **RNN controller**, which is trained using the REINFORCE with baseline
  algorithm.

![w=55%,h=center](nasnet_overview.svgz)

~~~
- For every generated sequence, the corresponding network is trained on CIFAR-10
  and the development accuracy is used as a return.

---
# Neural Architecture Search (NASNet) – 2017

The overall architecture of the designed network is fixed and only the Normal
Cells and Reduction Cells are generated by the controller.

![w=29%,h=center](nasnet_overall.svgz)

---
# Neural Architecture Search (NASNet) – 2017

- Each cell is composed of $B$ blocks ($B=5$ is used in NASNet).
~~~
- Each block is designed by a RNN controller generating 5 parameters.

![w=80%,h=center](nasnet_rnn_controller.svgz)

![w=60%,mw=50%,h=center](nasnet_block_steps.svgz)![w=80%,mw=50%,h=center](nasnet_operations.svgz)

- Every block is designed by a RNN controller generating individual operations.

---
# Neural Architecture Search (NASNet) – 2017

The final Normal Cell and Reduction Cell chosen from 20k architectures
(500GPUs, 4days).

![w=77%,h=center](nasnet_blocks.svgz)

---
section: EfficientNet
# EfficientNet Search

EfficientNet changes the search in three ways.

~~~
- Computational requirements are part of the return. Notably, the goal is to
  find an architecture $m$ maximizing
  $$\operatorname{DevelopmentAccuracy}(m) ⋅ \left(\frac{\textrm{TargetFLOPS=400M}}{\operatorname{FLOPS}(m)}\right)^{0.07},$$
~~~
  where the constant $0.07$ balances the accuracy and FLOPS (_the constant comes
  from an empirical observation that doubling the FLOPS brings about 5% relative
  accuracy gain, and $1.05 = 2^β$_ gives $β ≈ 0.0704$).

~~~
- Using a different search space, which allows to control kernel sizes and
  channels in different parts of the architecture (compared to using the same
  cell everywhere as in NASNet).

~~~
- Training directly on ImageNet, but only for 5 epochs.

~~~
In total, 8k model architectures are sampled, and PPO algorithm is used
instead of the REINFORCE with baseline.

---
# EfficientNet Search

![w=100%](mnasnet_overall.svgz)

![w=30%,f=right](mnasnet_parameters.svgz)

The overall architecture consists of 7 blocks, each described by 6 parameters
– 42 parameters in total, compared to 50 parameters of the NASNet search space.

---
# EfficientNet-B0 Baseline Network

![w=100%](../05/efficientnet_architecture.svgz)

---
section: WaveNet
# WaveNet

Our goal is to model speech, using an auto-regressive model
$$P(→x) = ∏_t P(x_t | x_{t-1}, …, x_1).$$

~~~
![w=80%,h=center](wavenet_causal_convolutions.svgz)

---
# WaveNet

![w=100%,v=middle](wavenet_dilated_convolutions.svgz)

---
# WaveNet

## Output Distribution
WaveNet generates audio with 16kHz frequency and 16-bit samples. However,
classification into $65\,536$ classes would not be tractable; instead, WaveNet
adopts $μ$-law transformation and quantize the samples into 256 values using
$$\sign(x)\frac{\ln(1 + 255|x|)}{\ln(1 + 255)}.$$

~~~
## Gated Activation
To allow greater flexibility, the outputs of the dilated convolutions are passed
through the gated activation units:
$$→z = \tanh(⇉W_f * →x) ⋅ σ(⇉W_g * →x).$$

---
# WaveNet

![w=80%,h=center](wavenet_block.svgz)

---
# WaveNet

## Global Conditioning
Global conditioning is performed by a single latent representation $→h$,
changing the gated activation function to
$$→z = \tanh(⇉W_f * →x + ⇉V_f→h) ⋅ σ(⇉W_g * →x + ⇉V_g→h).$$

~~~
## Local Conditioning
For local conditioning, we are given a time series $→h$, possibly with a lower
sampling frequency. We first use transposed convolutions $→y = f(→h)$ to match resolution
and then compute analogously to global conditioning
$$→z = \tanh(⇉W_f * →x + ⇉V_f * →y) ⋅ σ(⇉W_g * →x + ⇉V_g * →y).$$

---
# WaveNet

The original paper did not mention hyperparameters, but later it was revealed
that:
- 30 layers were used

~~~
  - grouped into 3 dilation stacks with 10 layers each
~~~
  - in a dilation stack, dilation rate increases by a factor of 2, starting
    with rate 1 and reaching maximum dilation of 512
~~~
- filter size of a dilated convolution is 3
~~~
- residual connection has dimension 512
~~~
- gating layer uses 256+256 hidden units
~~~
- the $1×1$ output convolution produces 256 filters
~~~
- trained for $1\,000\,000$ steps using Adam with a fixed learning rate of $0.0002$

---
# WaveNet

![w=85%,h=center](wavenet_results.svgz)

---
class: tablewide
style: table { line-height: 1.0 }
# Gated Activations in Transformers

Similar gated activations seem to work the best in Transformers, in the FFN
module.

~~~
| Activation Name | Formula | $\operatorname{FFN}(x; ⇉W_1, ⇉W_2)$ |
|:----------------|:-------:|:---:|
| ReLU            | $\max(0, x)$ | $\max(0, →x⇉W_1)⇉W_2$
~~~
| GELU            | $x Φ(x)$ | $\operatorname{GELU}(→x⇉W_1)⇉W_2$
~~~
| Swish           | $x σ(x)$ | $\operatorname{Swish}(→x⇉W_1)⇉W_2$
~~~

There are several variants of the new gated activations:

| Activation Name | Formula | $\operatorname{FFN}(x; ⇉W, ⇉V, ⇉W_2)$ |
|:----------------|:-------:|:---:|
| GLU (Gated Linear Unit) | $σ(→x⇉W + →b)⊙(→x⇉V + →c)$ | $(σ(→x⇉W)⊙→x⇉V)⇉W_2$
~~~
| ReGLU | $\max(0, →x⇉W + →b)⊙(→x⇉V + →c)$ | $(\max(0, →x⇉W)⊙→x⇉V)⇉W_2$
| GEGLU | $\operatorname{GELU}(→x⇉W + →b)⊙(→x⇉V + →c)$ | $(\operatorname{GELU}(→x⇉W)⊙→x⇉V)⇉W_2$
| SwiGLU | $\operatorname{Swish}(→x⇉W + →b)⊙(→x⇉V + →c)$ | $(\operatorname{Swish}(→x⇉W)⊙→x⇉V)⇉W_2$

---
# Gated Activations in Transformers

![w=73.8%](gated_transformers_glue.svgz)![w=26.2%](gated_transformers_squad.svgz)

![w=100%](gated_transformers.svgz)

---
section: ParallelWaveNet
# Parallel WaveNet

The output distribution was changed from 256 $μ$-law values to a Mixture of
Logistic (suggested in another paper – PixelCNN++, but reused in other architectures since):
$$x ∼ ∑_i π_i \operatorname{Logistic}(μ_i, s_i).$$

~~~
![w=28%,f=right](logistic_pdf.svgz)

The logistic distribution is a distribution with a $σ$ as cumulative density function
(where the mean and scale is parametrized by $μ$ and $s$). Therefore, we can
write
$$P(x | →π, →μ, →s) = ∑_i π_i \bigg[σ\Big(\frac{x + 0.5 - μ_i}{s_i}\Big) - σ\Big(\frac{x - 0.5 - μ_i}{s_i}\Big)\bigg],$$
where we replace $-0.5$ and $0.5$ in the edge cases by $-∞$ and $∞$.

~~~
In Parallel WaveNet, 10 mixture components are used.

---
# Parallel WaveNet

Auto-regressive (sequential) inference is extremely slow in WaveNet.

~~~
Instead, we will model $P(x_t)$ as $P(x_t | →z_{<t})
= \operatorname{Logistic}\big(x_t; μ^1(→z_{< t}), s^1(→z_{< t})\big)$
for a _random_ $→z$ drawn from a logistic distribution
$\operatorname{Logistic}(→0, →1)$. Then
$$x^1_t = z_t ⋅ s^1(→z_{< t}) + μ^1(→z_{< t}).$$

~~~
Usually, one iteration of the algorithm does not produce good enough results
– 4 iterations were used by the authors. In further iterations,
$$x^i_t = x^{i-1}_t ⋅ s^i(→x^{i-1}_{< t}) + μ^i(→x^{i-1}_{< t}).$$

~~~
After $N$ iterations, $P(→x^N_t | →z_{<t})$ is a logistic distribution
with location $→μ_\textrm{tot}$ and scale $→s_\textrm{tot}$:
$$→μ_\textrm{tot} = ∑_i^N μ^i(→x^{i-1}_{< t}) \cdot \Big(∏\nolimits_{j>i}^N s^j(→x^{j-1}_{< t})\Big)
\textrm{~~and~~}
→s_\textrm{tot} = ∏_i^N s^i(→x^{i-1}_{< t}).$$


---
# Probability Density Distillation

The network is trained using a **probability density distillation** using
a teacher WaveNet, using KL-divergence as loss.

![w=75%,h=center](parallel_wavenet_distillation.svgz)

---
# Probability Density Distillation

Denoting the teacher distribution as $P_T$ and the student distribution
as $P_S$, the loss is specifically
$$D_\textrm{KL}(P_S || P_T) = H(P_S, P_T) - H(P_S).$$

~~~
Therefore, we do not only minimize cross-entropy, but we also try to keep the
entropy of the student as high as possible. That is crucial not to match
just the mode of the teacher. (Consider a teacher generating white noise,
where every sample comes from $𝓝(0, 1)$ – in this case, the cross-entropy
loss of a constant $→0$, complete silence, would be maximal.)

~~~
In a sense, probability density distillation is similar to GANs. However,
the teacher is kept fixed and the student does not attempt to fool it
but to match its distribution instead.

---
class: dbend
# Probability Density Distillation Details

Because the entropy of a logistic distribution $\operatorname{Logistic}(μ, s)$
is $\ln s + 2$, the entropy term $H(P_S)$ can be rewritten as follows:
$$\begin{aligned}
H(P_S) &= 𝔼_{z∼\operatorname{Logistic}(0, 1)}\left[\sum_{t=1}^T - \ln p_S(x_t|→z_{<t})\right] \\
       &= 𝔼_{z∼\operatorname{Logistic}(0, 1)}\left[\sum_{t=1}^T \ln s(→z_{\lt t}, →θ)\right] + 2T.
\end{aligned}$$
Therefore, this term can be computed without having to generate $→x$.

---
class: dbend
# Probability Density Distillation Details

However, the cross-entropy term $H(P_S, P_T)$ requires sampling from $P_S$ to estimate:
$$\begin{aligned}
H(P_S, P_T) 
&= ∫_{→x} -P_S(→x) \ln P_T(→x) \\
&= ∑_{t=1}^T ∫_{→x} -P_S(→x) \ln P_T(x_t|→x_{<t}) \\
&= ∑_{t=1}^T ∫_{→x} -P_S(→x_{<t})P_S(x_t|→x_{<t})P_S(→x_{>t}|→x_{\leq t}) \ln P_T(x_t|→x_{<t}) \\
&= ∑_{t=1}^T 𝔼_{P_S(→x_{<t})} \bigg[∫_{x_t} -P_S(x_t|→x_{<t}) \ln P_T(x_t|→x_{<t})∫_{→x_{>t}} P_S(→x_{>t}|→x_{\leq t})\bigg] \\
&= ∑_{t=1}^T 𝔼_{P_S(→x_{<t})} H\Big(P_S(x_t|→x_{<t}), P_T(x_t|→x_{<t})\Big).
\end{aligned}$$

---
class: dbend
# Probability Density Distillation Details

$$H(P_S, P_T) = ∑_{t=1}^T 𝔼_{P_S(→x_{<t})} H\Big(P_S(x_t|→x_{<t}), P_T(x_t|→x_{<t})\Big)$$

We can therefore estimate $H(P_S, P_T)$ by drawing a single sample $→x$ from the
student $P_S$, compute all $P_T(x_t | →x_{<t})$ from the teacher in parallel,
and finally evaluate $H(P_S(x_t|→x_{<t}), P_T(x_t|→x_{<t}))$ by sampling
multiple different $x_t$ from the $P_S(x_t|→x_{<t})$; the authors state
that this unbiased estimator has a much lower variance than naively evaluating
the sample under the teacher using the original formulation.

~~~
Finally, analogously to the normal distribution, the logistic distribution
offers the **reparametric trick**, which means we can differentiate $\ln
P_T(x_t|→x_{<t})$ with respect to both $x_t$ and $→x_{<t}$ (while the
categorical distribution would be differentiable only with respect to
$→x_{< t}$).

---
# Parallel WaveNet

With the 4 iterations, the Parallel WaveNet generates over 500k samples per
second, compared to ~170 samples per second of a regular WaveNet – more than
a 1000 times speedup.

![w=80%,mh=80%,v=bottom,h=center](parallel_wavenet_results.svgz)

---
section: Tacotron
# Tacotron 2

![w=65%,h=center](tacotron.svgz)

---
# Tacotron 2

![w=65%,h=center,v=middle](tacotron_results.svgz)

---
class: middle
# Tacotron 2

![w=100%](tacotron_comparison.svgz)

You can listen to samples at https://google.github.io/tacotron/publications/tacotron2/

---
section: NTM
# Neural Turing Machines

So far, all input information was stored either directly in network weights, or
in a state of a recurrent network.

~~~
However, mammal brains seem to operate with a **working memory** – a capacity for
short-term storage of information and its rule-based manipulation.

~~~
We can therefore try to introduce an external memory to a neural network. The
memory $⇉M$ will be a matrix, where rows correspond to memory cells.

---
# Neural Turing Machines

The network will control the memory using a controller which reads from the
memory and writes to is. Although the original paper also considered
a feed-forward (non-recurrent) controller, usually the controller is a recurrent
LSTM network.

![w=55%,h=center](ntm_architecture.svgz)

---
# Neural Turing Machine

## Reading

To read the memory in a differentiable way, the controller at time $t$ emits
a read distribution $→w_t$ over memory locations, and the returned read vector $→r_t$
is then
$$→r_t = ∑_i w_t(i) ⋅ →M_t(i).$$

## Writing

Writing is performed in two steps – an **erase** followed by an **add**: the
controller at time $t$ emits a write distribution $→w_t$ over memory locations,
together with an _erase vector_ $→e_t$ and an _add vector_ $→a_t$. The memory is then
updated as
$$→M_t(i) = →M_{t-1}(i)\big[1 - w_t(i)→e_t] + w_t(i) →a_t.$$

---
# Neural Turing Machine

The addressing mechanism is designed to allow both
- content addressing, and
- location addressing.

![w=90%,h=center](ntm_addressing.svgz)

---
# Neural Turing Machine

## Content Addressing

Content addressing starts by the controller emitting the _key vector_ $→k_t$,
which is compared to all memory locations $→M_t(i)$, generating a distribution
using a $\softmax$ with temperature $β_t$.
$$w_t^c(i) = \frac{\exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(i))}{∑_j \exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(j))}$$

The $\operatorname{distance}$ measure is usually the cosine similarity
$$\operatorname{distance}(→a, →b) = \frac{→a^T →b}{||→a|| ⋅ ||→b||}.$$

---
# Neural Turing Machine

## Location-Based Addressing

To allow iterative access to memory, the controller might decide to reuse the
memory location from the previous timestep. Specifically, the controller emits
an _interpolation gate_ $g_t$ and sets
$$→w_t^g = g_t →w_t^c + (1 - g_t) →w_{t-1}.$$

Then, the current weighting may be shifted, i.e., the controller might decide to
“rotate” the weights by a small integer. For a given range (the simplest case
are only shifts $\{-1, 0, 1\}$), the network emits a $\softmax$ distribution over
the shifts, and the weights are then defined using a circular convolution
$$w̃_t(i) = ∑_j w_t^g(j) s_t(i - j).$$

Finally, not to lose precision over time, the controller emits
a _sharpening factor_ $γ_t$, and the final memory location weights are
$w_t(i) = {w̃_t(i)^{γ_t}} / {∑_j w̃_t(j)^{γ_t}}.$

---
# Neural Turing Machine

## Overall Execution

Even if not specified in the original paper, following the DNC paper, the LSTM
controller can be implemented as a (potentially deep) LSTM. Assuming $R$ read
heads and one write head, the input is $→x_t$ and $R$ read
vectors $→r_{t-1}^1, …, →r_{t-1}^R$ from the previous time step, the output of the
controller are vectors $(→ν_t, →ξ_t)$, and the final output is
$→y_t = →ν_t + ⇉W_r\big[→r_t^1, …, →r_t^R\big]$. The $→ξ_t$ is a concatenation of
$$→k_t^1, β_t^1, g_t^1, →s_t^1, γ_t^1, →k_t^2, β_t^2, g_t^2, →s_t^2, γ_t^2, …, →k_t^w, β_t^w, g_t^w, →s_t^w, γ_t^w, →e_t^w, →a_t^w.$$

---
# Neural Turing Machines

## Copy Task

Repeat the same sequence as given on input. Trained with sequences of length up
to 20.

![w=70%,h=center](ntm_copy_training.svgz)

---
# Neural Turing Machines

![w=84%,h=center](ntm_copy_generalization.svgz)

---
# Neural Turing Machines

![w=95%,h=center](ntm_copy_generalization_lstm.svgz)

---
# Neural Turing Machines

![w=65%,h=center](ntm_copy_memory.svgz)

---
# Neural Turing Machines

## Associative Recall

In associative recall, a sequence is given on input, consisting of subsequences
of length 3. Then a randomly chosen subsequence is presented on input and the
goal is to produce the following subsequence.

![w=65%,h=center](ntm_associative_recall_training.svgz)

---
# Neural Turing Machines

![w=83%,h=center](ntm_associative_recall_generalization.svgz)

---
# Neural Turing Machines

![w=53%,h=center](ntm_associative_recall_memory.svgz)

---
section: DNC
# Differentiable Neural Computer

NTM was later extended to a Differentiable Neural Computer.

![w=82%,h=center](dnc_architecture.svgz)

---
# Differentiable Neural Computer

The DNC contains multiple read heads and one write head.

~~~
The controller is a deep LSTM network, with input at time $t$ being the current
input $→x_t$ and $R$ read vectors $→r_{t-1}^1, …, →r_{t-1}^R$ from previous time
step. The output of the controller are vectors $(→ν_t, →ξ_t)$, and the final
output is $→y_t = →ν_t + W_r\big[→r_t^1, …, →r_t^R\big]$. The $→ξ_t$ is
a concatenation of parameters for read and write heads (keys, gates, sharpening
parameters, …).

~~~
In DNC, the usage of every memory location is tracked, which enables performing
dynamic allocation – at each time step, a cell with least usage can be allocated.

~~~
Furthermore, for every memory location, we track which memory location was
written to previously and subsequently, allowing to recover sequences in the
order in which it was written, independently on the real indices used.

~~~
The write weighting is defined as a weighted combination of the allocation
weighting and write content weighting, and read weighting is computed as a weighted
combination of read content weighting, previous write weighting, and subsequent
write weighting.


---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks.svgz)

---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks_traversal.svgz)

---
section: MANN
# Memory-augmented Neural Networks

External memory can be also utilized for **learning to learn**. Consider a network,
which should learn classification into a user-defined hierarchy by observing
ideally a small number of samples.

~~~
Apart from finetuning the model and storing the information in the _weights_,
an alternative is to store the samples in **external memory**. Therefore, the
model learns how to store the data and access it efficiently, which allows
it to learn without changing its weights.

![w=100%](mann_overview.svgz)

---
# Memory-augmented NNs

![w=90%,mw=62%](mann_reading.svgz)![w=38%](mann_writing.svgz)

---
# Memory-augmented NNs

![w=60%,h=center](mann_results.svgz)
