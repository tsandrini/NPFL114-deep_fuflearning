title: NPFL114, Lecture 2
class: title, langtech, cc-by-nc-sa
style: .algorithm { background-color: #eee; padding: .5em }
# Training Neural Networks

## Milan Straka

### February 21, 2022

---
# Refresh – Neural Networks

- Neural network describes a computation, which gets an input tensor and
  produces an output.

~~~
  - The input tensor has usually a fixed size.
~~~
  - The input tensor is usually a vector, but it can be 2D/3D/4D
    - images, video, time sequences like speech, …
~~~
  - The output usually describes a distribution
    - normal for regression
    - Bernoulli for binary classification
    - categorical for multiclass classification

~~~
- The basic units are _nodes_, composed in a acyclic graph.

~~~
- The edges have weights, nodes have activation functions.

~~~
- Nodes of neural networks are usually composed in layers.

---
section: ML Basics
# Machine Learning Basics

We usually have a **training set**, which is assumed to consist of examples
generated independently from a **data generating distribution**.

~~~
The goal of _optimization_ is to match the training set as well as possible.

~~~
However, the goal of _machine learning_ is to perform well on _previously
unseen_ data, to achieve lowest **generalization error** or **test error**. We
typically estimate it using a **test set** of examples independent of the
training set, but generated by the same data generating distribution.

---
# Machine Learning Basics

Challenges in machine learning:
- _underfitting_
- _overfitting_

~~~
![w=80%,h=center](underfitting_overfitting.svgz)

---
# Machine Learning Basics

We can control whether a model underfits or overfits by modifying its _capacity_.
~~~
- representational capacity
- effective capacity

~~~
![w=60%,h=center](generalization_error.svgz)

~~~
The **No free lunch theorem** (Wolpert, 1996) states that averaging over
_all possible_ data distributions, every classification algorithm achieves
the same _overall_ error when processing unseen examples. In a sense, no machine
learning algorithm is _universally_ better than others.

---
# Machine Learning Basics

Any change in a machine learning algorithm that is designed to _reduce
generalization error_ but not necessarily its training error is called
**regularization**.

~~~

$L_2$ regularization (also called weighted decay) penalizes models
with large weights (i.e., penalty of $\|→θ\|^2$).

![w=70%,h=center](regularization.svgz)

---
# Machine Learning Basics

**Hyperparameters** are not adapted by the learning algorithm itself.

Usually a **validation set** or **development set** is used to
estimate the generalization error, allowing to update hyperparameters accordingly.

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](double_descent.svgz)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent.svgz)

---
# Why do Neural Networks Generalize so Well

![w=90%,h=center](deep_double_descent_width.svgz)

---
# Why do Neural Networks Generalize so Well

![w=100%,v=middle](deep_double_descent_size_epochs.svgz)

---
section: Loss
# Loss Function

A model is usually trained in order to minimize the **loss** on the training data.

~~~

Assuming that a model computes $f(→x;→θ)$ using parameters $→θ$,
the **mean square error** of given $N$ examples $\big(→x^{(1)}, y^{(1)}\big),
\big(→x^{(2)}, y^{(2)}\big), …, \big(→x^{(N)}, y^{(N)}\big)$ is computed as
$$\frac{1}{N} ∑_{i=1}^N \Big(f(→x^{(i)}; →θ) - y^{(i)}\Big)^2.$$

~~~
A common principle used to design loss functions is the **maximum likelihood
principle**.

---
# Maximum Likelihood Estimation

Let $𝕏 = \{→x^{(1)}, →x^{(2)}, …, →x^{(N)}\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$.

~~~
We denote the **empirical data distribution** as $p̂_\textrm{data}$, where
$$p̂_\textrm{data}(→x) ≝ \frac{\big|\{i: →x^{(i)} = →x\}\big|}{N}.$$

~~~
Let $p_\textrm{model}(→x; →θ)$ be a family of distributions.
~~~
- If the weights are fixed, $p_\textrm{model}(→x{\color{lightgray}; →θ})$ is a probability distribution.
~~~
- If we instead consider the fixed training data $𝕏$, then
  $$L(→θ) = p_\textrm{model}(𝕏; →θ) = ∏\nolimits_{i=1}^N p_\textrm{model}(→x^{(i)}; →θ)$$
  is called the **likelihood**.
~~~
  Note that even if the value of the likelihood is in range $[0, 1]$, it is not
  a probability, because the likelihood is not a probability distribution.

---
# Maximum Likelihood Estimation

Let $𝕏 = \{→x^{(1)}, →x^{(2)}, …, →x^{(N)}\}$ be training data drawn
independently from the data-generating distribution $p_\textrm{data}$. We denote
the empirical data distribution as $p̂_\textrm{data}$ and let
$p_\textrm{model}(→x; →θ)$ be a family of distributions.

The **maximum likelihood estimation** of $→θ$ is:

$\displaystyle \kern14em\mathllap{→θ_\mathrm{MLE} = \argmax_{→θ} p_\textrm{model}(𝕏; →θ)} = \argmax_{→θ} ∏\nolimits_{i=1}^N p_\textrm{model}(→x^{(i)}; →θ)$

~~~
$\displaystyle \kern14em{} = \argmin_{→θ} ∑\nolimits_{i=1}^N -\log p_\textrm{model}(→x^{(i)}; →θ)$

~~~
$\displaystyle \kern14em{} = \argmin_{→θ} 𝔼_{⁇→x ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(→x; →θ)]$

~~~
$\displaystyle \kern14em{} = \argmin_{→θ} H(p̂_\textrm{data}(→x), p_\textrm{model}(→x; →θ))$

~~~
$\displaystyle \kern14em{} = \argmin_{→θ} D_\textrm{KL}(p̂_\textrm{data}(→x)\|p_\textrm{model}(→x; →θ)) \color{gray} + H(p̂_\textrm{data})$

---
# Maximum Likelihood Estimation

MLE can be easily generalized to a conditional case, where our goal is to predict $y$ given $→x$:
$$\begin{aligned}
→θ_\mathrm{MLE} = \argmax_{→θ} p_\textrm{model}(𝕐 | 𝕏; →θ) &= \argmin_{→θ} ∑\nolimits_{i=1}^N -\log p_\textrm{model}(y^{(i)} | →x^{(i)}; →θ) \\
               &= \argmin_{→θ} 𝔼_{(⁇→x, ⁇y) ∼ p̂_\textrm{data}} [-\log p_\textrm{model}(y | →x; →θ)] \\
               &= \argmin_{→θ} H(p̂_\textrm{data}, p_\textrm{model}(y | →x; →θ)) \\
               &= \argmin_{→θ} D_\textrm{KL}(p̂_\textrm{data}\|p_\textrm{model}(y | →x; →θ)) \color{gray} + H(p̂_\textrm{data})
\end{aligned}$$

~~~
where the conditional entropy is defined as
$H(p̂_\textrm{data}) = 𝔼_{(⁇→x, ⁇y) ∼ p̂_\textrm{data}} [-\log (p̂_\textrm{data}(y | →x; →θ))]$
and the conditional crossentropy as
$H(p̂_\textrm{data}, p_\textrm{model}) = 𝔼_{(⁇→x, ⁇y) ∼ p̂_\textrm{data}} [-\log (p_\textrm{model}(y | →x; →θ))]$.

~~~
The resulting _loss function_ is called **negative log-likelihood**, or
**cross-entropy** or **Kullback-Leibler divergence**.

---
# Estimators and Bias

An **estimator** is a rule for computing an estimate of a given value, often an
expectation of some random value(s).

~~~
The **bias** of an estimator is the difference of the expected value of the estimator
and the true value being estimated.

~~~
If the bias is zero, we call the estimator **unbiased**, otherwise we call it
**biased**.

~~~
If we have a sequence of estimates, it also might happen that the bias converges
to zero. Consider the well known sample estimate of variance. Given $⁇x_1,
…, ⁇x_N$ independent and identically distributed random variables, we might
estimate mean and variance as
$$μ̂ = \frac{1}{N} ∑\nolimits_i x_i,~~~σ̂^2 = \frac{1}{N} ∑\nolimits_i (x_i - μ̂)^2.$$
~~~
Such an estimate is biased, because $𝔼[σ̂^2] = (1 - \frac{1}{N})σ^2$, but the bias
converges to zero with increasing $N$.

~~~
Also, an unbiased estimator does not necessarily have small variance – in some
cases it can have large variance, so a biased estimator with smaller variance
might be preferred.

---
# Properties of Maximum Likelihood Estimation

Assume that the true data generating distribution $p_\textrm{data}$ lies within the model
family $p_\textrm{model}(⋅; →θ)$, and assume there exists a unique
$→θ_{p_\textrm{data}}$ such that $p_\textrm{data} = p_\textrm{model}(⋅; →θ_{p_\textrm{data}})$.

~~~
- MLE is a _consistent_ estimator. If we denote $→θ_m$ to be the parameters
  found by MLE for a training set with $m$ examples generated by the data
  generating distribution, then $→θ_m$ converges in probability to
  $→θ_{p_\textrm{data}}$.

  Formally, for any $ε > 0$, $P(\|→θ_m - →θ_{p_\textrm{data}}\| > ε) → 0$
  as $m → ∞$.

~~~
- MLE is in a sense the _most statistically efficient_. For any consistent estimator,
  let us consider the average distance of $→θ_m$ and $→θ_{p_\textrm{data}}$:
  $𝔼_{⁇→x_1, …, ⁇→x_m ∼ p_\textrm{data}} \big[\|→θ_m - →θ_{p_\textrm{data}}\|^2\big]$. \
  It can be shown (Rao 1945, Cramér 1946) that no consistent estimator has
  lower mean squared error than the maximum likelihood estimator.

~~~
Therefore, for reasons of consistency and efficiency, maximum likelihood is
often considered the preferred estimator for machine learning.

---
# Mean Square Error as MLE

![w=25%,f=right](constant_variance.svgz)

Assume our goal is to perform regression, i.e., to predict $p(y | →x)$ for $y ∈ ℝ$.
~~~
Let $ŷ(→x; →θ)$ give a prediction of the mean of $y$.

~~~
We define $p(y | →x)$ as $𝓝(y; ŷ(→x; →θ), σ^2)$ for a given fixed $σ^2$.
~~~
Then:
$$\begin{aligned}
\argmax_{→θ} p(y | 𝕏; →θ)
  =& \argmin_{→θ} ∑_{i=1}^N -\log p(y^{(i)} | →x^{(i)} ; →θ) \\
  =& \argmin_{→θ} -∑_{i=1}^N \log \sqrt{\frac{1}{2πσ^2}} e ^ {\normalsize -\frac{(y^{(i)} - ŷ(→x^{(i)}; →θ))^2}{2σ^2}} \\
  =& \argmin_{→θ} {\color{gray} -N \log (2πσ^2)^{-1/2}} - ∑_{i=1}^N -\frac{(y^{(i)} - ŷ(→x^{(i)}; →θ))^2}{2σ^2} \\
  =& \argmin_{→θ} ∑_{i=1}^N \frac{(y^{(i)} - ŷ(→x^{(i)}; →θ))^2}{2σ^2}
  = \argmin_{→θ} \frac{1}{N} ∑_{i=1}^N \Big(ŷ(→x^{(i)}; →θ) - y^{(i)}\Big)^2.
\end{aligned}$$

---
section: Gradient Descent
# Gradient Descent

Let a model compute $f(→x;→θ)$ using parameters $→θ$, and for a given loss
function $L$ denote
$$J(→θ) = 𝔼_{(→x, y)∼p̂_\textrm{data}} L\big(f(→x; →θ), y\big).$$
~~~
![w=53%,f=right](gradient_descent.svgz)

Assuming we are minimizing an error function
$$\argmin_{→θ} J(→θ)$$
we may use _gradient descent_:
$$→θ ← →θ - α∇_{→θ} J(→θ)$$

~~~
The constant $α$ is called a **learning rate** and specifies the “length”
of a step we perform in every iteration of the gradient descent.

---
# Gradient Descent Variants

## (Standard/Batch) Gradient Descent

We use all training data to compute $∇_{→θ} J(→θ)$.

~~~
## Stochastic (or Online) Gradient Descent

We estimate the expectation in $∇_{→θ} J(→θ)$ using a single randomly sampled example
from the training data. Such an estimate is unbiased, but very noisy.

$$∇_{→θ} J(→θ) ≈ ∇_{→θ} L\big(f(→x; →θ), y\big)\textrm{~~for randomly chosen~~}(→x, y)\textrm{~~from~~}p̂_\textrm{data}.$$

~~~
## Minibatch SGD

The minibatch SGD is a trade-off between gradient descent and SGD – the
expectation in $∇_{→θ} J(→θ)$ is estimated using $m$ random independent examples from
the training data.

$$∇_{→θ} J(→θ) ≈ \frac{1}{m} ∑_{i=1}^m ∇_{→θ} L\big(f(→x^{(i)}; →θ), y^{(i)}\big)
          \textrm{~~for randomly chosen~~}(→x^{(i)}, y^{(i)})\textrm{~~from~~}p̂_\textrm{data}.$$

---
# Stochastic Gradient Descent Convergence

Assume that we perform a stochastic gradient descent, using a sequence
of learning rates $α_i$, and using a noisy estimate of the real gradient
$∇_{→θ} J(→θ)$:
$$→θ_{i+1} ← →θ_i - α_i J(→θ_i).$$

~~~
It can be proven (under some reasonable conditions; see Robbins and Monro
algorithm, 1951) that if the loss function is convex and continuous, then SGD
converges to the unique optimum almost surely if the sequence of learning rates
$α_i$ fulfills the following conditions:
$$∀i: α_i > 0,~~~∑_i α_i = ∞,~~~∑_i α_i^2 < ∞.$$

~~~
Note that the third condition implies that $α_i → 0$.

~~~
For non-convex loss functions, we can get guarantees of converging to a _local_
optimum only.

~~~
Note that finding a global minimum of an arbitrary function is _at least NP-hard_.

---
# Stochastic Gradient Descent Convergence

Convex functions mentioned on the previous slide are such that for $x_1, x_2$
and real $0 ≤ t ≤ 1$,
$$f(tx_1 + (1-t)x_2) ≤ tf(x_1) + (1-t)f(x_2).$$

![w=90%,mw=50%,h=center](convex_2d.svgz)![w=68%,mw=50%,h=center](convex_3d.svgz)

~~~
A twice-differentiable function is convex iff its second derivative is always
non-negative.

~~~
A local minimum of a convex function is always the unique global minimum.

~~~
Well-known examples of convex functions are $x^2$, $e^x$ and $-\log x$.

---
# Stochastic Gradient Descent Convergence

In 2018, there have been several improvements:
- Under some models with high capacity, it can be proven that SGD will reach
  global optimum by showing it will reach zero training error.
~~~

- Neural networks can be easily modified so that the augmented version has
  no local minimums. Therefore, if such a network converges, it converged
  to a global minimum. However, the training process can still fail to converge
  by increasing the size of the parameters $\|→θ\|$ beyond any limit.


---
class: wide
# Loss Function Visualization

Visualization of loss function of ResNet-56 (0.85 million parameters)
with/without skip connections:
![w=100%](nn_loss.jpg)

---
class: wide
# Loss Function Visualization

Visualization of loss function of ResNet-110 without skip connections and DenseNet-121:
![w=100%](nn_loss_densenet.jpg)

---
section: Backprop
# Backpropagation

Assume we want to compute partial derivatives of a given loss function $J$ and
let $\frac{∂J}{∂z}$ be known.

![w=30%,h=center](chain_rule.svgz)

$$\begin{gathered}
\frac{∂J}{∂y_i} = \frac{∂J}{∂z} \frac{∂z}{∂y_i} = \frac{∂J}{∂z} \frac{∂g(→y)}{∂y_i} \\
\frac{∂J}{∂→x_i} = \frac{∂J}{∂z} \frac{∂z}{∂y_i} \frac{∂y_i}{∂→x_i} = \frac{∂J}{∂z} \frac{∂g(→y)}{∂y_i} \frac{∂f(→x_i)}{∂→x_i}
\end{gathered}$$

---
# Backpropagation Example

![w=75%,h=center,v=middle](net.svgz)

---
# Backpropagation Example

![w=75%,h=center,v=middle](net-forward.svgz)

---
# Backpropagation Example

![w=100%,mh=95%,v=middle](net-backward.svgz)

~~~
This is meant to be frightening – you do **not** do this manually when training.

---
class: middle
# Backpropagation Algorithm

#### Forward Propagation
<div class="algorithm">

**Input**: Network with nodes $u^{(1)}, u^{(2)}, …, u^{(n)}$ numbered in
topological order.  
Each node's value is computed as $u^{(i)} = f^{(i)}(\mathbb A^{(i)})$
for $\mathbb A^{(i)}$ being a set of values of the predecessors $P(u^{(i)})$ of
$u^{(i)}$. <br>
**Output**: Value of $u^{(n)}$.
- For $i = 1, …, n$:
    - $\mathbb A^{(i)} ← \lbrace u^{(j)} | j ∈ P(u^{(i)})\rbrace$
    - $u^{(i)} ← f^{(i)}(\mathbb A^{(i)})$
- Return $u^{(n)}$
</div>

---
class: middle
# Backpropagation Algorithm

#### Simple Variant of Backpropagation
<div class="algorithm">

**Input**: The network as in the Forward propagation algorithm.<br>
**Output**: Partial derivatives $g^{(i)} = \frac{∂u^{(n)}}{∂u^{(i)}}$ of $u^{(n)}$ with respect to all $u^{(i)}$.
- Run forward propagation to compute all $u^{(i)}$
- $g^{(n)} = 1$
- For $i = n-1, …, 1$:
    - $g^{(i)} ← ∑_{j:i∈P(u^{(j)})} g^{(j)} \frac{∂u^{(j)}}{∂u^{(i)}}$
- Return $→g$
</div>

In practice, we do not usually represent networks as collections of scalar
nodes; instead we represent them as collections of tensor functions – most
usually functions $f: ℝ^n → ℝ^m$. Then $\frac{∂f(→x)}{∂→x}$ is a Jacobian.
However, the backpropagation algorithm is analogous.

---
section: NN Training
# Neural Network Architecture à la '80s

![w=45%,h=center](../01/neural_network.svg)

---
# Neural Network Architecture à la '80s
There is a weight on each edge, and an activation function $f$ is performed on the
hidden layers, and optionally also on the output layer.
$$h_i = f\left(∑_j w_{i,j} x_j + b_i\right)$$

If the network is composed of layers, we can use matrix notation and write:
$$→h = f\left(⇉W →x + →b\right)$$

---
# Neural Network Activation Functions

## Hidden Layers Derivatives
- $σ$:
  $$\frac{dσ(x)}{dx} = σ(x) ⋅ (1-σ(x))$$
~~~
- $\tanh$:
  $$\frac{d\tanh(x)}{dx} = 1 - \tanh(x)^2$$
~~~
- ReLU:
  $$ \frac{d\ReLU(x)}{dx} = \begin{cases} 1 &\text{if } x > 0 \\ \textrm{NaN} &\text{if }x = 0 \mathrlap{\textrm{; 0 is usually used}} \\ 0 &\text{if } x < 0 \end{cases}$$

---
section: SGDs
class: middle
# Stochastic Gradient Descent

#### Stochastic Gradient Descent (SGD) Algorithm
<div class="algorithm">

**Input**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.  
**Input**: Learning rate $α$.  
**Output**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇_{→θ} ∑_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→θ ← →θ - α→g$
</div>

---
# SGD With Momentum

#### SGD With Momentum
<div class="algorithm">
![w=40%,f=right](momentum.svgz)

**Input**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.  
**Input**: Learning rate $α$, momentum $β$.  
**Output**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇_{→θ} ∑_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→v ← β→v - α→g$
    - $→θ ← →θ + →v$
</div>

A nice writeup about how and why momentum works can be found on https://distill.pub/2017/momentum/.

---
# SGD With Nesterov Momentum

#### SGD With Nesterov Momentum
<div class="algorithm">
![w=55%,f=right](nesterov.jpg)

**Input**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.<br>
**Input**: Learning rate $α$, momentum $β$.<br>
**Output**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→θ ← →θ + β→v$
    - $→g ← \frac{1}{m} ∇_{→θ} ∑_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→v ← β→v - α→g$
    - $→θ ← →θ - α→g$
</div>

---
section: Adaptive LR
class: middle
# Algorithms with Adaptive Learning Rates

#### AdaGrad (2011)
<div class="algorithm">

**Input**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.  
**Input**: Learning rate $α$, constant $ε$ (usually $10^{-8}$).  
**Output**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇_{→θ} ∑_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→r ← →r + →g^2$
    - $→θ ← →θ - \frac{α}{\sqrt{→r + ε}}→g$
</div>

- The $→g^2$ and $\frac{α}{\sqrt{→r + ε}}→g$ are computed element-wise.
- The $→g^2$ is sometimes also written as $→g⊙→g$.

---
# Algorithms with Adaptive Learning Rates

AdaGrad has favourable convergence properties (being faster than regular SGD)
for convex loss landscapes. In this settings, gradients converge to zero
reasonably fast.

~~~
However, for non-convex losses, gradients can stay quite large for a long time.
In that case, the algorithm behaves as if decreasing learning rate by a factor
of $1/\sqrt{t}$, because if each
$$→g ≈ →g_0,$$
then after $t$ steps
$$→r ≈ t ⋅ →g_0^2$$
and therefore
$$\frac{α}{\sqrt{→r + ε}} ≈ \frac{α / \sqrt{t}}{\sqrt{→g_0^2 + ε/t}}.$$

---
# Algorithms with Adaptive Learning Rates

#### RMSProp (2012)
<div class="algorithm">

**Input**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.  
**Input**: Learning rate $α$, momentum $β$ (usually $0.9$), constant $ε$ (usually $10^{-8}$).  
**Output**: Updated parameters $→θ$.
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇_{→θ} ∑_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $→r ← β→r + (1-β)→g^2$
    - $→θ ← →θ - \frac{α}{\sqrt{→r + ε}}→g$
</div>

~~~
However, after first step, $→r = (1-β)→g^2$, which for default $β=0.9$ is
$$→r = 0.1 →g^2,$$
a biased estimate (but the bias converges to zero exponentially fast).

---
# Algorithms with Adaptive Learning Rates

#### Adam (2014)
<div class="algorithm">

**Input**: NN computing function $f(→x; →θ)$ with initial value of parameters $→θ$.  
**Input**: Learning rate $α$ (default 0.001), constant $ε$ (usually $10^{-8}$).  
**Input**: Momentum $β_1$ (default 0.9), momentum $β_2$ (default 0.999).  
**Output**: Updated parameters $→θ$.
- $→s ← 0$, $→r ← 0$, $t ← 0$
- Repeat until stopping criterion is met:
    - Sample a minibatch of $m$ training examples $(→x^{(i)}, y^{(i)})$
    - $→g ← \frac{1}{m} ∇_{→θ} ∑_i L(f(→x^{(i)}; →θ), y^{(i)})$
    - $t ← t + 1$
    - $→s ← β_1→s + (1-β_1)→g$               _(biased first moment estimate)_
    - $→r ← β_2→r + (1-β_2)→g^2$              _(biased second moment estimate)_
    - $→ŝ ← →s / (1 - β_1^t)$, $→r̂ ← →r / (1 - β_2^t)$    _(unbiased momenta estimates)_
    - $→θ ← →θ - \frac{α}{\sqrt{→r̂ + ε}}→ŝ$
</div>

---
# Adam Bias Correction

To allow analysis, we add indices to the update
$$→s_{t} ← β_1→s_{t-1} + (1 - β_1)→g_t,$$
with $→s_0 ← 0$.

~~~ ~
# Adam Bias Correction

To allow analysis, we add indices to the update
$$→s_{t} ← β_1→s_{t-1} + (1 - β_1)→g_t,$$

![w=50%,f=right](ema_all.svgz)

with $→s_0 ← 0$.

After $t$ steps, we have

$$→s_t = (1 - β_1) ∑_{i=1}^t β_1^{t-i}→g_i.$$

~~~
Because $∑_{i=0}^∞ β_1^i = \frac{1}{1 - β_1}$, $→s_∞$ is computed as a weighted
average of infinitely many elements.

---
# Adam Bias Correction

![w=50%,f=right](ema_truncated.svgz)

However, for $t < ∞$, the sum of weights in the computation of $→s_t$ does not
sum to one.

~~~
To obtain an unbiased estimate, we therefore need to account for the “missing”
elements; in other words, we need to scale the weights, so that they sum
to one.

~~~
The sum of weights after $t$ steps is
$$(1 - β_1) ∑_{i=1}^t β_1^{t-i} = ∑_{i=1}^t β_1^{t-i} - ∑_{i=0}^{t-1} β_1^{t-i} = 1 - β_1^t,$$

~~~
so we obtain an unbiased estimate by dividing $→s_t$ with $(1 - β_1^t)$, and
analogously for the correction of $→r$.

---
# Adaptive Optimizers Animations

![w=50%,h=center](optimizers-1.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-2.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-3.gif)

---
# Adaptive Optimizers Animations

![w=65%,h=center](optimizers-4.gif)

---
section: LR Schedules
# Learning Rate Schedules

Even if RMSProp and Adam are adaptive, they still usually require carefully tuned
decreasing learning rate for top-notch performance.

~~~
![w=32%,f=right](decay_linear.svgz)

- **Polynomial decay**: learning rate is multiplied by some polynomial of $t$.
  - **Linear decay** uses $α = α_\textrm{initial} ⋅ \big(1 - \frac{t}{\textrm{max~steps}}\big)$ and has
    theoretical guarantees of convergence, but is usually too fast for deep
    neural networks.
~~~
  - **Inverse square root decay** uses $α = α_\textrm{initial} ⋅ \frac{1}{\sqrt{t}}$
    and is currently used by best machine translation models.

~~~
![w=32%,f=right](decay_exponential.svgz)

- **Exponential decay**: learning rate is multiplied by a constant each
  minibatch/epoch/several epochs.

  - $α = α_\textrm{initial} ⋅ c^t$
  - Often used for convolutional networks (image recognition etc.).

---
section: LR Schedules
# Learning Rate Schedules

![w=32%,f=right](decay_cosine.svgz)

- **Cosine decay**: The cosine decay has became quite popular in the past years,
  both for training and finetuning.

  $$\frac{1}{2}\bigg(1 + \cos\Big(π ⋅ \frac{t}{\textrm{max~steps}}\Big)\bigg)$$

~~~
- Cyclic restarts, warmup, …

~~~
The `tf.optimizers.schedules` offers several such learning rate schedules,
which can be passed to any Keras optimizer directly as a learning rate.
- `tf.optimizers.schedules.PolynomialDecay`
- `tf.optimizers.schedules.ExponentialDecay`
- `tf.optimizers.schedules.CosineDecay`
